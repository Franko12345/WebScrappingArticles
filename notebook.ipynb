{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## NSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "    from selenium import webdriver\n",
    "\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.wait import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    if input(\"Deseja rodar preparação de ambiente (RECOMENDADO PARA PRIMEIRA VEZ): [sim/não]\") in [\"sim\", \"Sim\", \"S\", \"s\"]:\n",
    "        raise Exception(\"Preparação de ambiente solicitada\")\n",
    "except:\n",
    "    print(\"Configurando ambiente\")\n",
    "    \n",
    "    import os\n",
    "    import subprocess\n",
    "    \n",
    "    print(\"Checking for not installed packages...\")\n",
    "    \n",
    "    result = subprocess.run([\"pip\", \"list\"], stdout=subprocess.PIPE, text=True)\n",
    "\n",
    "    if not all([lib in result.stdout for lib in [\"selenium\",\"wget\",\"pandas\",\"openpyxl\", \"beautifulsoup4\"]]):\n",
    "        print(\"Installing packages...\")\n",
    "        os.system(\"pip install --upgrade selenium wget pandas openpyxl beautifulsoup4\")\n",
    "    \n",
    "    print(\"All packages are installed!\")\n",
    "    \n",
    "    \n",
    "    print(\"Checking for outdated packages...\")\n",
    "    result = subprocess.run([\"pip\", \"list\", \"--outdated\"], stdout=subprocess.PIPE, text=True)\n",
    "    \n",
    "    if any([lib in result.stdout for lib in [\"selenium\",\"wget\",\"pandas\",\"openpyxl\", \"beautifulsoup4\"]]):\n",
    "        print(\"Updating packages...\")\n",
    "        os.system(\"pip install --upgrade selenium wget pandas openpyxl beautifulsoup4\")\n",
    "\n",
    "    print(\"All packages are updated!\")\n",
    "    \n",
    "    import wget\n",
    "    import zipfile\n",
    "    \n",
    "    if \"chromedriver\" not in os.listdir():\n",
    "        print(\"Downloading chromedriver\")\n",
    "        filename = wget.download(\"https://storage.googleapis.com/chrome-for-testing-public/134.0.6998.165/win64/chromedriver-win64.zip\")\n",
    "        with zipfile.ZipFile(f\"./{filename}\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"./chromedriver\")\n",
    "    else:\n",
    "        print(\"Chromedriver found!\")\n",
    "    \n",
    "    if \"chrome\" not in os.listdir():\n",
    "        print(\"Downloading chrome\")\n",
    "        filename = wget.download(\"https://storage.googleapis.com/chrome-for-testing-public/134.0.6998.165/win64/chrome-win64.zip\")\n",
    "        with zipfile.ZipFile(f\"./{filename}\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"./chrome\")\n",
    "    else:\n",
    "        print(\"Chrome found!\")\n",
    "\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.support.wait import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "options.binary_location = \"./chrome/chrome-win64/chrome.exe\"\n",
    "\n",
    "driverpath = Service(\"./chromedriver/chromedriver-win64/chromedriver.exe\")\n",
    "\n",
    "options.add_argument('--headless=new')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')  # Evita problemas de memória compartilhada\n",
    "options.add_argument('--disable-web-security')\n",
    "options.add_argument('--disable-site-isolation-trials')\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--allow-running-insecure-content')\n",
    "options.add_argument('--disable-notifications')\n",
    "\n",
    "options.page_load_strategy = 'eager'\n",
    "\n",
    "driver = webdriver.Chrome(service=driverpath, options=options)\n",
    "\n",
    "def articleFormatter(article, tag): \n",
    "    return {\n",
    "        \"title\": article.find(\"h3\").text.strip(),\n",
    "        \"link\": article.find(\"a\").get_attribute_list(\"href\")[0],\n",
    "        \"data\": article.find(\"div\", class_=\"date\").text.strip(),\n",
    "        \"tag\": tag\n",
    "    }\n",
    "\n",
    "def SCfilter(article):\n",
    "    cidades_sc1 = pd.read_excel('./planilhas/cidade_sc1.xlsx')\n",
    "    \n",
    "    for key in [\" sc \", \"santa catarina\", \" sc\", \"sc \"]:\n",
    "        if key in article[\"title\"].lower():\n",
    "            return True\n",
    "        \n",
    "        \n",
    "    for key in [\"-sc-\", \"santa-catarina\", \"-sc\", \"sc-\"]:\n",
    "        if key in article[\"link\"].split(\"/\")[-1].lower():\n",
    "            return True\n",
    "\n",
    "    for cidade in cidades_sc1[\"MUNICIPIO\"]:\n",
    "        if cidade.lower() in article[\"title\"].lower() or cidade.lower() in article[\"link\"].split(\"/\")[-1].lower():\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def getNewsByTags(tags):\n",
    "    global driver\n",
    "    allNews = []\n",
    "\n",
    "    pageCounter = 0\n",
    "    for tag in tags.keys():\n",
    "        for page in range(tags[tag]):\n",
    "            if pageCounter % 10 == 0:\n",
    "                print(f\"\\nPlanilha salva com {len(allNews)} notícias para backup...\")\n",
    "                storeAsExcel(allNews)\n",
    "                print(\"Salvo\\n\")\n",
    "            \n",
    "            acessed = False\n",
    "            \n",
    "            while not acessed:\n",
    "                try:\n",
    "                    driver.get(f\"https://www.nsctotal.com.br/tag/{tag}?page={page}\")\n",
    "                    acessed = True\n",
    "                except:\n",
    "                    print(\"Erro ao acessar a página, reiniciando navegador...\")\n",
    "                    driver.quit()\n",
    "                    driver = webdriver.Chrome(service=driverpath, options=options)\n",
    "    \n",
    "            try:\n",
    "                WebDriverWait(driver, 10).until( EC.presence_of_element_located( (By.CLASS_NAME, \"date\") ) )\n",
    "                driver.implicitly_wait(5)\n",
    "            except:\n",
    "                continue            \n",
    "            \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            news = soup.find_all('div', class_='featured-news-thumb')\n",
    "            \n",
    "            parsedNews = [articleFormatter(article, tag) for article in news]\n",
    "\n",
    "            parsedNews = list(filter(SCfilter, parsedNews))\n",
    "            \n",
    "            allNews += parsedNews\n",
    "            print(f\"\\nNoticias coletadas: {len(parsedNews)}\\nTag: {tag}\\nPágina:{page}\\nTotal: {len(allNews)}\\n\")\n",
    "\n",
    "            pageCounter += 1\n",
    "        \n",
    "        \n",
    "            \n",
    "    return allNews\n",
    "\n",
    "def storeAsExcel(data):\n",
    "    rows = list(map(lambda article: article.values(), data))\n",
    "    df = pd.DataFrame(rows, columns=[\"title\", \"link\", \"data\", \"tag\"])\n",
    "    \n",
    "    print(f\"Número de noticias com duplicados: {len(df)}\")\n",
    "    \n",
    "    df = df.drop(\"tag\", axis=1)\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    print(f\"Número de noticias sem duplicados: {len(df)}\")\n",
    "    \n",
    "    df.to_excel(\"./planilhas/noticias.xlsx\", index=False)\n",
    "    \n",
    "\n",
    "searchReference = {\n",
    "    \"chuvas\": 52,\n",
    "    \"chuva em sc\": 35,\n",
    "    \"chuvas em sc\": 60,\n",
    "    \"chuva\": 395,\n",
    "    \"chuva forte\": 5,\n",
    "    \"chuvarada\": 11,\n",
    "    \"temporal\": 109,\n",
    "    \"tempestade\": 21,\n",
    "    \"ciclone\": 32,\n",
    "    \"ciclone bomba\": 5,\n",
    "    \"ciclone extratropical\": 6,\n",
    "    \"previsao do tempo\": 708,\n",
    "    \"frente fria\": 9,\n",
    "    \"enchente\": 92,\n",
    "    \"enchentes\": 22,\n",
    "    \"alagamento\": 61,\n",
    "    \"alagamentos\": 20,\n",
    "    \"deslizamento\": 36,\n",
    "    \"deslizamentos\": 9,\n",
    "    \"deslizamento de terra\": 5\n",
    "}\n",
    "\n",
    "searchReference = {tag.replace(\" \", \"-\"): searchReference[tag] for tag in searchReference}\n",
    "\n",
    "data = getNewsByTags(searchReference)\n",
    "\n",
    "print(data)\n",
    "\n",
    "storeAsExcel(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NDMAIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "    from selenium import webdriver\n",
    "\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.wait import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    if input(\"Deseja rodar preparação de ambiente (RECOMENDADO PARA PRIMEIRA VEZ): [sim/não]\") in [\"sim\", \"Sim\", \"S\", \"s\"]:\n",
    "        raise Exception(\"Preparação de ambiente solicitada\")\n",
    "except:\n",
    "    print(\"Configurando ambiente\")\n",
    "    \n",
    "    import os\n",
    "    import subprocess\n",
    "    \n",
    "    print(\"Checking for not installed packages...\")\n",
    "    \n",
    "    result = subprocess.run([\"pip\", \"list\"], stdout=subprocess.PIPE, text=True)\n",
    "\n",
    "    if not all([lib in result.stdout for lib in [\"selenium\",\"wget\",\"pandas\",\"openpyxl\", \"beautifulsoup4\"]]):\n",
    "        print(\"Installing packages...\")\n",
    "        os.system(\"pip install --upgrade selenium wget pandas openpyxl beautifulsoup4\")\n",
    "    \n",
    "    print(\"All packages are installed!\")\n",
    "    \n",
    "    \n",
    "    print(\"Checking for outdated packages...\")\n",
    "    result = subprocess.run([\"pip\", \"list\", \"--outdated\"], stdout=subprocess.PIPE, text=True)\n",
    "    \n",
    "    if any([lib in result.stdout for lib in [\"selenium\",\"wget\",\"pandas\",\"openpyxl\", \"beautifulsoup4\"]]):\n",
    "        print(\"Updating packages...\")\n",
    "        os.system(\"pip install --upgrade selenium wget pandas openpyxl beautifulsoup4\")\n",
    "\n",
    "    print(\"All packages are updated!\")\n",
    "    \n",
    "    import wget\n",
    "    import zipfile\n",
    "    \n",
    "    if \"chromedriver\" not in os.listdir():\n",
    "        print(\"Downloading chromedriver\")\n",
    "        filename = wget.download(\"https://storage.googleapis.com/chrome-for-testing-public/134.0.6998.165/win64/chromedriver-win64.zip\")\n",
    "        with zipfile.ZipFile(f\"./{filename}\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"./chromedriver\")\n",
    "    else:\n",
    "        print(\"Chromedriver found!\")\n",
    "    \n",
    "    if \"chrome\" not in os.listdir():\n",
    "        print(\"Downloading chrome\")\n",
    "        filename = wget.download(\"https://storage.googleapis.com/chrome-for-testing-public/134.0.6998.165/win64/chrome-win64.zip\")\n",
    "        with zipfile.ZipFile(f\"./{filename}\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"./chrome\")\n",
    "    else:\n",
    "        print(\"Chrome found!\")\n",
    "\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.support.wait import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "options.binary_location = \"./chrome/chrome-win64/chrome.exe\"\n",
    "\n",
    "driverpath = Service(\"./chromedriver/chromedriver-win64/chromedriver.exe\")\n",
    "\n",
    "# options.add_argument('--headless=new')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')  # Evita problemas de memória compartilhada\n",
    "options.add_argument('--disable-web-security')\n",
    "options.add_argument('--disable-site-isolation-trials')\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--allow-running-insecure-content')\n",
    "options.add_argument('--disable-notifications')\n",
    "\n",
    "options.page_load_strategy = 'eager'\n",
    "\n",
    "driver = webdriver.Chrome(service=driverpath, options=options)\n",
    "\n",
    "def articleFormatter(article, tag): \n",
    "    return {\n",
    "        \"title\": article.find(\"a\").get_attribute_list(\"title\")[0].strip(),\n",
    "        \"link\": article.find(\"a\").get_attribute_list(\"href\")[0],\n",
    "        \"data\": article.find(\"time\").get_attribute_list(\"title\")[0].strip(),\n",
    "        \"tag\": tag\n",
    "    }\n",
    "\n",
    "def SCfilter(article):\n",
    "    cidades_sc1 = pd.read_excel('./planilhas/cidade_sc1.xlsx')\n",
    "    \n",
    "    for key in [\" sc \", \"santa catarina\", \" sc\", \"sc \"]:\n",
    "        if key in article[\"title\"].lower():\n",
    "            return True\n",
    "        \n",
    "        \n",
    "    for key in [\"-sc-\", \"santa-catarina\", \"-sc\", \"sc-\"]:\n",
    "        if key in article[\"link\"].split(\"/\")[-1].lower():\n",
    "            return True\n",
    "\n",
    "    for cidade in cidades_sc1[\"MUNICIPIO\"]:\n",
    "        if cidade.lower() in article[\"title\"].lower() or cidade.lower() in article[\"link\"].split(\"/\")[-1].lower():\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def getNewsByTags(tags):\n",
    "    global driver\n",
    "    allNews = []\n",
    "\n",
    "    for tag in tags:\n",
    "        \n",
    "        acessed = False\n",
    "        \n",
    "        while not acessed:\n",
    "            try:\n",
    "                driver.get(f\"https://ndmais.com.br/?s={tag}\")\n",
    "                acessed = True\n",
    "            except:\n",
    "                print(\"Erro ao acessar a página, reiniciando navegador...\")\n",
    "                driver.quit()\n",
    "                driver = webdriver.Chrome(service=driverpath, options=options)\n",
    "\n",
    "        WebDriverWait(driver, 10).until( EC.presence_of_element_located( (By.CLASS_NAME, \"title-text\")))\n",
    "\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                ActionChains(driver).scroll_by_amount(0, 10000).perform()\n",
    "                WebDriverWait(driver, 10).until( EC.presence_of_element_located( (By.CSS_SELECTOR, \"#loop-content > div.site-button > a\") ) )\n",
    "                ActionChains(driver).scroll_by_amount(0, 10000).perform()\n",
    "                driver.implicitly_wait(5)\n",
    "                ActionChains(driver).scroll_by_amount(0, 10000).perform()\n",
    "                driver.find_element(By.CSS_SELECTOR, \"#loop-content > div.site-button > a\").click()\n",
    "                ActionChains(driver).scroll_by_amount(0, 10000).perform()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break            \n",
    "\n",
    "        ###CONCERTAR ESSA PARTE O ELEMENTO AS VEZES NÃO É CLICAVEL OU ENCONTADO\n",
    "        \n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        news = soup.find_all('div', class_='site-card-content')\n",
    "        \n",
    "        parsedNews = [articleFormatter(article, tag) for article in news]\n",
    "\n",
    "        parsedNews = list(filter(SCfilter, parsedNews))\n",
    "        \n",
    "        allNews += parsedNews\n",
    "\n",
    "\n",
    "        print(f\"\\nPlanilha salva com {len(allNews)} notícias para backup...\")\n",
    "        storeAsExcel(allNews)\n",
    "        print(\"Salvo\\n\")\n",
    "            \n",
    "        print(f\"\\nNoticias coletadas: {len(parsedNews)}\\nTag: {tag}\\nTotal: {len(allNews)}\\n\")\n",
    "        \n",
    "            \n",
    "    return allNews\n",
    "\n",
    "def storeAsExcel(data):\n",
    "    rows = list(map(lambda article: article.values(), data))\n",
    "    df = pd.DataFrame(rows, columns=[\"title\", \"link\", \"data\", \"tag\"])\n",
    "    \n",
    "    print(f\"Número de noticias com duplicados: {len(df)}\")\n",
    "    \n",
    "    df = df.drop(\"tag\", axis=1)\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    print(f\"Número de noticias sem duplicados: {len(df)}\")\n",
    "    \n",
    "    df.to_excel(\"./planilhas/noticias.xlsx\", index=False)\n",
    "    \n",
    "\n",
    "searchReference = {\n",
    "    \"chuvas\",\n",
    "    \"chuva em sc\",\n",
    "    \"chuvas em sc\",\n",
    "    \"chuva\",\n",
    "    \"chuva forte\",\n",
    "    \"chuvarada\",\n",
    "    \"temporal\",\n",
    "    \"tempestade\",\n",
    "    \"ciclone\",\n",
    "    \"ciclone bomba\",\n",
    "    \"ciclone extratropical\",\n",
    "    \"previsão do tempo\",\n",
    "    \"frente fria\",\n",
    "    \"enchente\",\n",
    "    \"enchentes\",\n",
    "    \"alagamento\",\n",
    "    \"alagamentos\",\n",
    "    \"deslizamento\",\n",
    "    \"deslizamentos\",\n",
    "    \"deslizamento de terra\"\n",
    "}\n",
    "\n",
    "searchReference = list(map(lambda x: x.replace(\" \", \"+\"), searchReference))\n",
    "\n",
    "data = getNewsByTags(searchReference)\n",
    "\n",
    "print(data)\n",
    "\n",
    "storeAsExcel(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificador de notícias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import os\n",
    "    import math\n",
    "    import pandas as pd\n",
    "    from dotenv import load_dotenv\n",
    "    from google import genai\n",
    "except:\n",
    "    print(\"Configurando ambiente\")\n",
    "    \n",
    "    os.system(\"pip install --upgrade google-genai python-dotenv\")\n",
    "\n",
    "    from dotenv import load_dotenv\n",
    "    from google import genai\n",
    "    \n",
    "load_dotenv()\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "def generateContent(prompt):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=prompt,   \n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "def classifyTitles(data):\n",
    "    return generateContent(\"Vou te passar titulos de noticias e quero que você classifique simplesmente como \\\"previsão\\\", para coisas que ainda não aconteceram, \\\"aconteceu\\\" para coisas que já aconteceram e \\\"historico\\\" para coisas que já aconteceram e não vão mais acontecer. Não quero explicações, apenas a classificação. Exemplo: \\\"Amanhã vai chover em SC\\\" = previsão. Exemplo 2: \\\"Ontem choveu em SC\\\" = aconteceu. Exemplo 3: \\\"Em 2000 choveu em SC\\\" = historico. A formatação devera ser a seguinte <Titulo da notícia> = <Classificação>\\n Titulos: \" + \"\\n\".join(data))\n",
    "\n",
    "def dispatchTitles(articles, groupSize=30):\n",
    "    titleClassification = []\n",
    "\n",
    "    for i in range(0, math.ceil(len(articles)/groupSize)):\n",
    "        start = i * groupSize\n",
    "        end = (i + 1) * groupSize\n",
    "        \n",
    "        payload = classifyTitles([article for article in articles[start:end]]).split(\"\\n\")[:-1]\n",
    "        payload = [{x.split(\" = \")[0] : x.split(\" = \")[1]} for x in payload]\n",
    "        titleClassification += payload\n",
    "        \n",
    "        print(f\"{min(end,len(articles))} de {len(articles)} classificações concluídas\")\n",
    "\n",
    "    return titleClassification\n",
    "    \n",
    "    \n",
    "articles = pd.read_excel('./planilhas/noticias.xlsx')\n",
    "lookupArticleIndex = {x:i for i, x in enumerate(list(articles[\"title\"]))}\n",
    "\n",
    "classifiedTitles = dispatchTitles(list(articles[\"title\"]))\n",
    "\n",
    "classifiedNews = []\n",
    "\n",
    "for title in classifiedTitles:\n",
    "    newsIndex = lookupArticleIndex[list(title.keys())[0]]\n",
    "    classifiedNews.append({\n",
    "        \"title\": articles[\"title\"][newsIndex],\n",
    "        \"link\": articles[\"link\"][newsIndex],\n",
    "        \"data\": articles[\"data\"][newsIndex],\n",
    "        \"classificacao\": list(title.values())[0]\n",
    "    })\n",
    "    \n",
    "print(\"Salvando em planilha...\")\n",
    "df = pd.DataFrame(classifiedNews, columns=classifiedNews[0].keys())\n",
    "df.to_excel(\"./planilhas/noticias_classificadas.xlsx\", index=False)\n",
    "print(\"Salvo com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
