{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## NSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "    from selenium import webdriver\n",
    "\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.wait import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    if input(\"Deseja rodar preparação de ambiente (RECOMENDADO PARA PRIMEIRA VEZ): [sim/não]\") in [\"sim\", \"Sim\", \"S\", \"s\"]:\n",
    "        raise Exception(\"Preparação de ambiente solicitada\")\n",
    "except:\n",
    "    print(\"Configurando ambiente\")\n",
    "    \n",
    "    import os\n",
    "    import subprocess\n",
    "    \n",
    "    print(\"Checking for not installed packages...\")\n",
    "    \n",
    "    result = subprocess.run([\"pip\", \"list\"], stdout=subprocess.PIPE, text=True)\n",
    "\n",
    "    if not all([lib in result.stdout for lib in [\"selenium\",\"wget\",\"pandas\",\"openpyxl\", \"beautifulsoup4\"]]):\n",
    "        print(\"Installing packages...\")\n",
    "        os.system(\"pip install --upgrade selenium wget pandas openpyxl beautifulsoup4\")\n",
    "    \n",
    "    print(\"All packages are installed!\")\n",
    "    \n",
    "    \n",
    "    print(\"Checking for outdated packages...\")\n",
    "    result = subprocess.run([\"pip\", \"list\", \"--outdated\"], stdout=subprocess.PIPE, text=True)\n",
    "    \n",
    "    if any([lib in result.stdout for lib in [\"selenium\",\"wget\",\"pandas\",\"openpyxl\", \"beautifulsoup4\"]]):\n",
    "        print(\"Updating packages...\")\n",
    "        os.system(\"pip install --upgrade selenium wget pandas openpyxl beautifulsoup4\")\n",
    "\n",
    "    print(\"All packages are updated!\")\n",
    "    \n",
    "    import wget\n",
    "    import zipfile\n",
    "    \n",
    "    if \"chromedriver\" not in os.listdir():\n",
    "        print(\"Downloading chromedriver\")\n",
    "        filename = wget.download(\"https://storage.googleapis.com/chrome-for-testing-public/134.0.6998.165/win64/chromedriver-win64.zip\")\n",
    "        with zipfile.ZipFile(f\"./{filename}\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"./chromedriver\")\n",
    "    else:\n",
    "        print(\"Chromedriver found!\")\n",
    "    \n",
    "    if \"chrome\" not in os.listdir():\n",
    "        print(\"Downloading chrome\")\n",
    "        filename = wget.download(\"https://storage.googleapis.com/chrome-for-testing-public/134.0.6998.165/win64/chrome-win64.zip\")\n",
    "        with zipfile.ZipFile(f\"./{filename}\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"./chrome\")\n",
    "    else:\n",
    "        print(\"Chrome found!\")\n",
    "\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.support.wait import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "options.binary_location = \"./chrome/chrome-win64/chrome.exe\"\n",
    "\n",
    "driverpath = Service(\"./chromedriver/chromedriver-win64/chromedriver.exe\")\n",
    "\n",
    "options.add_argument('--headless=new')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')  # Evita problemas de memória compartilhada\n",
    "options.add_argument('--disable-web-security')\n",
    "options.add_argument('--disable-site-isolation-trials')\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--allow-running-insecure-content')\n",
    "options.add_argument('--disable-notifications')\n",
    "\n",
    "options.page_load_strategy = 'eager'\n",
    "\n",
    "driver = webdriver.Chrome(service=driverpath, options=options)\n",
    "\n",
    "def articleFormatter(article, tag): \n",
    "    return {\n",
    "        \"title\": article.find(\"h3\").text.strip(),\n",
    "        \"link\": article.find(\"a\").get_attribute_list(\"href\")[0],\n",
    "        \"data\": article.find(\"div\", class_=\"date\").text.strip(),\n",
    "        \"tag\": tag\n",
    "    }\n",
    "\n",
    "def SCfilter(article):\n",
    "    cidades_sc1 = pd.read_excel('./planilhas/cidade_sc1.xlsx')\n",
    "    \n",
    "    for key in [\" sc \", \"santa catarina\", \" sc\", \"sc \"]:\n",
    "        if key in article[\"title\"].lower():\n",
    "            return True\n",
    "        \n",
    "        \n",
    "    for key in [\"-sc-\", \"santa-catarina\", \"-sc\", \"sc-\"]:\n",
    "        if key in article[\"link\"].split(\"/\")[-1].lower():\n",
    "            return True\n",
    "\n",
    "    for cidade in cidades_sc1[\"MUNICIPIO\"]:\n",
    "        if cidade.lower() in article[\"title\"].lower() or cidade.lower() in article[\"link\"].split(\"/\")[-1].lower():\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def getNewsByTags(tags):\n",
    "    global driver\n",
    "    allNews = []\n",
    "\n",
    "    pageCounter = 0\n",
    "    for tag in tags.keys():\n",
    "        for page in range(tags[tag]):\n",
    "            if pageCounter % 10 == 0:\n",
    "                print(f\"\\nPlanilha salva com {len(allNews)} notícias para backup...\")\n",
    "                storeAsExcel(allNews)\n",
    "                print(\"Salvo\\n\")\n",
    "            \n",
    "            acessed = False\n",
    "            \n",
    "            while not acessed:\n",
    "                try:\n",
    "                    driver.get(f\"https://www.nsctotal.com.br/tag/{tag}?page={page}\")\n",
    "                    acessed = True\n",
    "                except:\n",
    "                    print(\"Erro ao acessar a página, reiniciando navegador...\")\n",
    "                    driver.quit()\n",
    "                    driver = webdriver.Chrome(service=driverpath, options=options)\n",
    "    \n",
    "            try:\n",
    "                WebDriverWait(driver, 10).until( EC.presence_of_element_located( (By.CLASS_NAME, \"date\") ) )\n",
    "                driver.implicitly_wait(5)\n",
    "            except:\n",
    "                continue            \n",
    "            \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            news = soup.find_all('div', class_='featured-news-thumb')\n",
    "            \n",
    "            parsedNews = [articleFormatter(article, tag) for article in news]\n",
    "\n",
    "            parsedNews = list(filter(SCfilter, parsedNews))\n",
    "            \n",
    "            allNews += parsedNews\n",
    "            print(f\"\\nNoticias coletadas: {len(parsedNews)}\\nTag: {tag}\\nPágina:{page}\\nTotal: {len(allNews)}\\n\")\n",
    "\n",
    "            pageCounter += 1\n",
    "        \n",
    "        \n",
    "            \n",
    "    return allNews\n",
    "\n",
    "def storeAsExcel(data):\n",
    "    rows = list(map(lambda article: article.values(), data))\n",
    "    df = pd.DataFrame(rows, columns=[\"title\", \"link\", \"data\", \"tag\"])\n",
    "    \n",
    "    print(f\"Número de noticias com duplicados: {len(df)}\")\n",
    "    \n",
    "    df = df.drop(\"tag\", axis=1)\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    print(f\"Número de noticias sem duplicados: {len(df)}\")\n",
    "    \n",
    "    df.to_excel(\"./planilhas/noticias.xlsx\", index=False)\n",
    "    \n",
    "\n",
    "searchReference = {\n",
    "    \"chuvas\": 52,\n",
    "    \"chuva em sc\": 35,\n",
    "    \"chuvas em sc\": 60,\n",
    "    \"chuva\": 395,\n",
    "    \"chuva forte\": 5,\n",
    "    \"chuvarada\": 11,\n",
    "    \"temporal\": 109,\n",
    "    \"tempestade\": 21,\n",
    "    \"ciclone\": 32,\n",
    "    \"ciclone bomba\": 5,\n",
    "    \"ciclone extratropical\": 6,\n",
    "    \"previsao do tempo\": 708,\n",
    "    \"frente fria\": 9,\n",
    "    \"enchente\": 92,\n",
    "    \"enchentes\": 22,\n",
    "    \"alagamento\": 61,\n",
    "    \"alagamentos\": 20,\n",
    "    \"deslizamento\": 36,\n",
    "    \"deslizamentos\": 9,\n",
    "    \"deslizamento de terra\": 5\n",
    "}\n",
    "\n",
    "searchReference = {tag.replace(\" \", \"-\"): searchReference[tag] for tag in searchReference}\n",
    "\n",
    "data = getNewsByTags(searchReference)\n",
    "\n",
    "print(data)\n",
    "\n",
    "storeAsExcel(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NDMAIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from httpx import TimeoutException\n",
    "from tqdm import tqdm\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "    from selenium import webdriver\n",
    "\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.wait import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "    import pandas as pd\n",
    "    import time\n",
    "\n",
    "\n",
    "    if input(\"Deseja rodar preparação de ambiente (RECOMENDADO PARA PRIMEIRA VEZ): [sim/não]\") in [\"sim\", \"Sim\", \"S\", \"s\"]:\n",
    "        raise Exception(\"Preparação de ambiente solicitada\")\n",
    "except:\n",
    "    print(\"Configurando ambiente\")\n",
    "    \n",
    "    import os\n",
    "    import subprocess\n",
    "    \n",
    "    print(\"Checking for not installed packages...\")\n",
    "    \n",
    "    result = subprocess.run([\"pip\", \"list\"], stdout=subprocess.PIPE, text=True)\n",
    "\n",
    "    if not all([lib in result.stdout for lib in [\"selenium\",\"wget\",\"pandas\",\"openpyxl\", \"beautifulsoup4\"]]):\n",
    "        print(\"Installing packages...\")\n",
    "        os.system(\"pip install --upgrade selenium wget pandas openpyxl beautifulsoup4\")\n",
    "    \n",
    "    print(\"All packages are installed!\")\n",
    "    \n",
    "    \n",
    "    print(\"Checking for outdated packages...\")\n",
    "    result = subprocess.run([\"pip\", \"list\", \"--outdated\"], stdout=subprocess.PIPE, text=True)\n",
    "    \n",
    "    if any([lib in result.stdout for lib in [\"selenium\",\"wget\",\"pandas\",\"openpyxl\", \"beautifulsoup4\"]]):\n",
    "        print(\"Updating packages...\")\n",
    "        os.system(\"pip install --upgrade selenium wget pandas openpyxl beautifulsoup4\")\n",
    "\n",
    "    print(\"All packages are updated!\")\n",
    "    \n",
    "    import wget\n",
    "    import zipfile\n",
    "    \n",
    "    if \"chromedriver\" not in os.listdir():\n",
    "        print(\"Downloading chromedriver\")\n",
    "        filename = wget.download(\"https://storage.googleapis.com/chrome-for-testing-public/134.0.6998.165/win64/chromedriver-win64.zip\")\n",
    "        with zipfile.ZipFile(f\"./{filename}\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"./chromedriver\")\n",
    "    else:\n",
    "        print(\"Chromedriver found!\")\n",
    "    \n",
    "    if \"brave\" not in os.listdir():\n",
    "        print(\"Downloading brave\")\n",
    "        filename = wget.download(\"https://github.com/brave/brave-browser/releases/download/v1.76.82/brave-v1.76.82-win32-x64.zip\")\n",
    "        with zipfile.ZipFile(f\"./{filename}\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"./brave\")\n",
    "    else:\n",
    "        print(\"Brave found!\")\n",
    "\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.support.wait import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "options.binary_location = \"./brave/brave.exe\"\n",
    "\n",
    "driverpath = Service(\"./chromedriver/chromedriver-win64/chromedriver.exe\")\n",
    "\n",
    "# options.add_argument('--headless=new')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')  # Evita problemas de memória compartilhada\n",
    "options.add_argument('--disable-web-security')\n",
    "options.add_argument('--disable-site-isolation-trials')\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--allow-running-insecure-content')\n",
    "options.add_argument('--disable-notifications')\n",
    "\n",
    "options.page_load_strategy = 'eager'\n",
    "\n",
    "driver = webdriver.Chrome(service=driverpath, options=options)\n",
    "\n",
    "def articleFormatter(article, tag, progress_bar=None): \n",
    "    progress_bar.update(1)\n",
    "    return {\n",
    "        \"title\": article.find(\"a\").get_attribute_list(\"title\")[0].strip(),\n",
    "        \"link\": article.find(\"a\").get_attribute_list(\"href\")[0],\n",
    "        \"data\": article.find(\"time\").get_attribute_list(\"title\")[0].strip(),\n",
    "        \"tag\": tag\n",
    "    }\n",
    "\n",
    "def SCfilter(article):\n",
    "    cidades_sc1 = pd.read_excel('./planilhas/cidade_sc1.xlsx')\n",
    "    \n",
    "    for key in [\" sc \", \"santa catarina\", \" sc\", \"sc \"]:\n",
    "        if key in article[\"title\"].lower():\n",
    "            return True\n",
    "        \n",
    "        \n",
    "    for key in [\"-sc-\", \"santa-catarina\", \"-sc\", \"sc-\"]:\n",
    "        if key in article[\"link\"].split(\"/\")[-1].lower():\n",
    "            return True\n",
    "\n",
    "    for cidade in cidades_sc1[\"MUNICIPIO\"]:\n",
    "        if cidade.lower() in article[\"title\"].lower() or cidade.lower() in article[\"link\"].split(\"/\")[-1].lower():\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def getNewsByTags(tags):\n",
    "    global driver\n",
    "    allNews = []\n",
    "\n",
    "    for tag in tags:\n",
    "        \n",
    "        acessed = False\n",
    "        \n",
    "        while not acessed:\n",
    "            try:\n",
    "                driver.get(f\"https://ndmais.com.br/?s={tag}\")\n",
    "                acessed = True\n",
    "            except:\n",
    "                print(\"Erro ao acessar a página, reiniciando navegador...\")\n",
    "                driver.quit()\n",
    "                driver = webdriver.Chrome(service=driverpath, options=options)\n",
    "\n",
    "        WebDriverWait(driver, 10).until( EC.presence_of_element_located( (By.CLASS_NAME, \"title-text\")))\n",
    "\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                ActionChains(driver).scroll_by_amount(0, 10000).perform()\n",
    "                \n",
    "                time.sleep(1)\n",
    "                \n",
    "                WebDriverWait(driver, 10).until( EC.presence_of_element_located( (By.CSS_SELECTOR, \"a[title=\\\"Veja Mais\\\"]\") ) )\n",
    "                                \n",
    "                driver.find_element(By.CSS_SELECTOR, \"a[title=\\\"Veja Mais\\\"]\").click()\n",
    "\n",
    "                WebDriverWait(driver, 10).until_not( EC.visibility_of_element_located( (By.CSS_SELECTOR, \"i[class=\\\"button-icon fas fa-spin fa-sync nd-fa-loaded\\\"]\") ) )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"Page {tag} carregada completamente\")\n",
    "                break            \n",
    "\n",
    "        ###CONCERTAR ESSA PARTE O ELEMENTO AS VEZES NÃO É CLICAVEL OU ENCONTADO\n",
    "        \n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        news = soup.find_all('div', class_='site-card-content')\n",
    "        \n",
    "        # print(f\"Formatando notícias com a tag {tag}...\")\n",
    "        \n",
    "        progress_bar = tqdm(total=len(news), desc=f\"Formatando notícias com a tag {tag}\", unit=\"notícias\")\n",
    "        parsedNews = [articleFormatter(article, tag, progress_bar) for article in news]\n",
    "\n",
    "        print(\"Notícias formatadas!\")\n",
    "\n",
    "        print(f\"Filtrando notícias com a tag {tag}...\")\n",
    "        parsedNews = list(filter(SCfilter, parsedNews))\n",
    "                \n",
    "        print(\"Notícias Filtradas!\")\n",
    "        \n",
    "        allNews += parsedNews\n",
    "\n",
    "\n",
    "        print(f\"\\nPlanilha salva com {len(allNews)} notícias para backup...\")\n",
    "        storeAsExcel(allNews)\n",
    "        print(\"Salvo\\n\")\n",
    "            \n",
    "        print(f\"\\nNoticias coletadas: {len(parsedNews)}\\nTag: {tag}\\nTotal: {len(allNews)}\\n\")\n",
    "        \n",
    "            \n",
    "    return allNews\n",
    "\n",
    "def storeAsExcel(data):\n",
    "    rows = list(map(lambda article: article.values(), data))\n",
    "    df = pd.DataFrame(rows, columns=[\"title\", \"link\", \"data\", \"tag\"])\n",
    "    \n",
    "    print(f\"Número de noticias com duplicados: {len(df)}\")\n",
    "    \n",
    "    df = df.drop(\"tag\", axis=1)\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    print(f\"Número de noticias sem duplicados: {len(df)}\")\n",
    "    \n",
    "    df.to_excel(\"./planilhas/noticias.xlsx\", index=False)\n",
    "    \n",
    "\n",
    "searchReference = {\n",
    "    \"chuvas\",\n",
    "    \"chuva em sc\",\n",
    "    \"chuvas em sc\",\n",
    "    \"chuva\",\n",
    "    \"chuva forte\",\n",
    "    \"chuvarada\",\n",
    "    \"temporal\",\n",
    "    \"tempestade\",\n",
    "    \"ciclone\",\n",
    "    \"ciclone bomba\",\n",
    "    \"ciclone extratropical\",\n",
    "    \"previsão do tempo\",\n",
    "    \"frente fria\",\n",
    "    \"enchente\",\n",
    "    \"enchentes\",\n",
    "    \"alagamento\",\n",
    "    \"alagamentos\",\n",
    "    \"deslizamento\",\n",
    "    \"deslizamentos\",\n",
    "    \"deslizamento de terra\"\n",
    "}\n",
    "\n",
    "searchReference = list(map(lambda x: x.replace(\" \", \"+\"), searchReference))\n",
    "\n",
    "data = getNewsByTags(searchReference)\n",
    "\n",
    "print(data)\n",
    "\n",
    "storeAsExcel(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificador de notícias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import os\n",
    "    import math\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    from dotenv import load_dotenv\n",
    "    from google import genai\n",
    "except:\n",
    "    print(\"Configurando ambiente\")\n",
    "    \n",
    "    os.system(\"pip install --upgrade google-genai python-dotenv\")\n",
    "\n",
    "    from dotenv import load_dotenv\n",
    "    from google import genai\n",
    "    \n",
    "load_dotenv()\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "def generateContent(prompt):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=prompt,   \n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "def classifyTitles(data):\n",
    "    return generateContent(\"Vou te passar titulos de noticias e quero que você classifique simplesmente como \\\"previsão\\\", para coisas que ainda não aconteceram, \\\"aconteceu\\\" para coisas que já aconteceram e \\\"historico\\\" para coisas que já aconteceram e não vão mais acontecer. Não quero explicações nem que mude os titulos, apenas a classificação. Exemplo: \\\"Amanhã vai chover em SC\\\" = previsão. Exemplo 2: \\\"Ontem choveu em SC\\\" = aconteceu. Exemplo 3: \\\"Em 2000 choveu em SC\\\" = historico. A formatação devera ser a seguinte <Titulo da notícia exatamente como foi enviado> = <Classificação>\\n Titulos: \" + \"\\n\".join(data))\n",
    "\n",
    "def dispatchTitles(articles, groupSize=300):\n",
    "    titleClassification = []\n",
    "\n",
    "    for i in range(0, math.ceil(len(articles)/groupSize)):\n",
    "        start = i * groupSize\n",
    "        end = (i + 1) * groupSize\n",
    "        \n",
    "        payload = classifyTitles([str(pos+start)+\".\"+article for pos,article in enumerate(articles[start:end])]).split(\"\\n\")[:-1]\n",
    "        payload = [{x.split(\" = \")[0].replace(\"*\",\"\").strip() : x.split(\" = \")[1]} for x in payload]\n",
    "        print(payload)\n",
    "        print(len(payload))\n",
    "        titleClassification += payload\n",
    "        \n",
    "        print(f\"{min(end,len(articles))} de {len(articles)} classificações concluídas\")\n",
    "\n",
    "    return titleClassification\n",
    "\n",
    "def dispatchRemainingTitles(articles, indexes, groupSize=200):\n",
    "    titleClassification = []\n",
    "\n",
    "    for i in range(0, math.ceil(len(indexes)/groupSize)):\n",
    "        start = i * groupSize\n",
    "        end = (i + 1) * groupSize\n",
    "        \n",
    "        selectedArticles = [articles[pos] for pos in indexes[start:end]]\n",
    "        \n",
    "        payload = classifyTitles([str(indexes[pos+start])+\".\"+article for pos,article in enumerate(selectedArticles)]).split(\"\\n\")[:-1]\n",
    "        payload = [{x.split(\" = \")[0].replace(\"*\",\"\").strip() : x.split(\" = \")[1]} for x in payload]\n",
    "        print(payload)\n",
    "        print(len(payload))\n",
    "        titleClassification += payload\n",
    "        \n",
    "        print(f\"{min(end,len(indexes))} de {len(indexes)} classificações concluídas\")\n",
    "\n",
    "    print(titleClassification)\n",
    "    return titleClassification\n",
    "\n",
    "\n",
    "arquivoDeNoticias = input(\"Qual arquivo você quer classificar: \")\n",
    "articles = pd.read_excel(f'./planilhas/{arquivoDeNoticias}')\n",
    "\n",
    "#cleaning tht titles\n",
    "articles[\"title\"] = articles[\"title\"].apply(lambda x: x.replace(\" *\", \"\").replace(\"*\", \"\").replace(\"* \", \"\").replace(\"  \", \" \").replace(\"  \", \" \").replace(\"  \", \" \").replace(u'\\xa0', u' ').strip())\n",
    "\n",
    "#Crate lookup table for title and index\n",
    "lookupArticleIndex = {x:[] for x in list(articles[\"title\"])}\n",
    "[lookupArticleIndex[x].append(i) for i, x in enumerate(list(articles[\"title\"]))]\n",
    "\n",
    "print(lookupArticleIndex)\n",
    "\n",
    "useBackup = (True if input(\"Você quer usar o backup de titulos? [sim/não]\") == \"sim\" else False) if os.path.exists(\"backup_titles.json\") else False\n",
    "\n",
    "if (useBackup):\n",
    "    print(\"Lendo backup de titulos...\")\n",
    "    with open(\"backup_titles.json\", \"r\", encoding=\"utf-16\") as arq:\n",
    "        classifiedTitles = json.loads(arq.read())\n",
    "else:\n",
    "    classifiedTitles = dispatchTitles(list(articles[\"title\"]))\n",
    "\n",
    "    with open(\"backup_titles.json\", \"w\", encoding=\"utf-16\") as arq:\n",
    "        arq.write(json.dumps(classifiedTitles))\n",
    "\n",
    "coverage = set(map(lambda x: int(list(x.keys())[0].split(\".\")[0]) if \".\" in list(x.keys())[0][:5] else -1, classifiedTitles))\n",
    "print(\"Not covered indexes:\")\n",
    "notCovered = list(set(range(len(list(articles[\"title\"])))).difference(coverage))\n",
    "print(notCovered)\n",
    "\n",
    "while len(notCovered) > 0:\n",
    "    classifiedTitles += dispatchRemainingTitles(list(articles[\"title\"]), notCovered)\n",
    "    \n",
    "    print(classifiedTitles[-668:])\n",
    "    \n",
    "    coverage = set(map(lambda x: int(list(x.keys())[0].split(\".\")[0]) if \".\" in list(x.keys())[0][:7] else -1, classifiedTitles))\n",
    "    print(\"Not covered indexes:\")\n",
    "    notCovered = list(set(range(len(list(articles[\"title\"])))).difference(coverage))\n",
    "    print(notCovered)\n",
    "\n",
    "\n",
    "classifiedNews = []\n",
    "\n",
    "for title in classifiedTitles:\n",
    "    idx = int(list(title.keys())[0].split(\".\")[0])\n",
    "    \n",
    "    classifiedNews.append({\n",
    "        \"title\": articles[\"title\"][idx],\n",
    "        \"link\": articles[\"link\"][idx],\n",
    "        \"data\": articles[\"data\"][idx],\n",
    "        \"classificacao\": list(title.values())[0]\n",
    "    })\n",
    "\n",
    "print(\"Salvando em planilha...\")\n",
    "df = pd.DataFrame(classifiedNews, columns=classifiedNews[0].keys())\n",
    "df.to_excel(\"./planilhas/noticias_classificadas.xlsx\", index=False)\n",
    "print(\"Salvo com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"./planilhas/noticias.xlsx\")\n",
    "\n",
    "titles = list(df[\"title\"])\n",
    "\n",
    "lookupArticleIndex = {x:[] for x in list(articles[\"title\"])}\n",
    "[lookupArticleIndex[x].append(i) for i, x in enumerate(list(articles[\"title\"]))]\n",
    "\n",
    "print(articles[\"title\"][6310])\n",
    "\n",
    "# print(len(lookupArticleIndex.keys()))\n",
    "\n",
    "# print([x for x in titles if \"ondas de até 3 metros \" in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(len(json.loads(open(\"backup_titles.json\", \"r\", encoding=\"utf-16\").read())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
