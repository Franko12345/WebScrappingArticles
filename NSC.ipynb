{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d77044b1",
   "metadata": {},
   "source": [
    "# üìÑ Coletor de Not√≠cias NSC - Santa Catarina\n",
    "\n",
    "## üìÖ Descri√ß√£o\n",
    "\n",
    "Script automatizado para coleta de not√≠cias relacionadas a eventos clim√°ticos em **Santa Catarina**, diretamente do portal **NDMais**. Utiliza **Selenium** com o navegador **Brave**, e faz parsing do HTML via **BeautifulSoup**.\n",
    "\n",
    "Ideal para monitoramento de not√≠cias sobre chuvas, enchentes, ciclones e outros eventos extremos no estado.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Requisitos\n",
    "\n",
    "* Python 3.x\n",
    "* Bibliotecas Python:\n",
    "\n",
    "  * `selenium`\n",
    "  * `pandas`\n",
    "  * `tqdm`\n",
    "  * `wget`\n",
    "  * `beautifulsoup4`\n",
    "  * `openpyxl`\n",
    "\n",
    "O script verifica e instala automaticamente os pacotes acima, se necess√°rio.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Como Usar\n",
    "\n",
    "1. Clone o reposit√≥rio e execute o script principal:\n",
    "\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "2. Ao ser perguntado:\n",
    "\n",
    "```bash\n",
    "Deseja rodar prepara√ß√£o de ambiente (RECOMENDADO PARA PRIMEIRA VEZ): [sim/n√£o]\n",
    "```\n",
    "\n",
    "Digite `sim` na primeira execu√ß√£o para:\n",
    "\n",
    "* Instalar depend√™ncias\n",
    "* Baixar o ChromeDriver\n",
    "* Baixar o navegador Chrome (vers√£o para automa√ß√£o)\n",
    "\n",
    "3. O script iniciar√° a coleta de not√≠cias automaticamente.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Funcionalidades\n",
    "\n",
    "* Verifica√ß√£o e instala√ß√£o autom√°tica de bibliotecas e drivers\n",
    "* Download do Chrome e do ChromeDriver para automa√ß√£o\n",
    "* Coleta de not√≠cias por m√∫ltiplos termos e tags\n",
    "* Filtragem de not√≠cias com base em Santa Catarina\n",
    "* Rein√≠cio autom√°tico do navegador em caso de falha\n",
    "* Backup cont√≠nuo durante a coleta (a cada 10 p√°ginas)\n",
    "* Armazenamento dos dados em Excel, com remo√ß√£o de duplicatas\n",
    "\n",
    "---\n",
    "\n",
    "## üìÉ Estrutura dos Dados\n",
    "\n",
    "Cada not√≠cia coletada possui:\n",
    "\n",
    "* `title`: T√≠tulo da not√≠cia\n",
    "* `link`: URL completa\n",
    "* `data`: Data de publica√ß√£o\n",
    "* `tag`: Palavra-chave que originou a busca (removida no Excel final)\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ Diret√≥rios Esperados\n",
    "\n",
    "```plaintext\n",
    ".\n",
    "‚îú‚îÄ‚îÄ chromedriver/          # Gerado automaticamente\n",
    "‚îú‚îÄ‚îÄ chrome/                # Chrome para automa√ß√£o (gerado automaticamente)\n",
    "‚îú‚îÄ‚îÄ planilhas/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ cidade_sc1.xlsx    # Lista de munic√≠pios catarinenses (necess√°rio)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ noticias.xlsx      # Planilha gerada com as not√≠cias filtradas\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† L√≥gica de Funcionamento\n",
    "\n",
    "* As not√≠cias s√£o buscadas por meio de navega√ß√£o automatizada por tags (termos de busca)\n",
    "* A cada 10 p√°ginas visitadas, o progresso √© salvo\n",
    "* A fun√ß√£o `SCfilter()` verifica se a not√≠cia est√° relacionada ao estado de SC com base em:\n",
    "\n",
    "  * Termos como \"sc\", \"santa catarina\"\n",
    "  * Nome dos munic√≠pios encontrados na planilha `cidade_sc1.xlsx`\n",
    "* Not√≠cias duplicadas s√£o removidas antes de salvar em Excel\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Termos de Busca Utilizados\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"chuvas\": 52,\n",
    "  \"chuva em sc\": 35,\n",
    "  \"chuvas em sc\": 60,\n",
    "  \"chuva\": 395,\n",
    "  \"chuva forte\": 5,\n",
    "  \"chuvarada\": 11,\n",
    "  \"temporal\": 109,\n",
    "  \"tempestade\": 21,\n",
    "  \"ciclone\": 32,\n",
    "  \"ciclone bomba\": 5,\n",
    "  \"ciclone extratropical\": 6,\n",
    "  \"previsao do tempo\": 708,\n",
    "  \"frente fria\": 9,\n",
    "  \"enchente\": 92,\n",
    "  \"enchentes\": 22,\n",
    "  \"alagamento\": 61,\n",
    "  \"alagamentos\": 20,\n",
    "  \"deslizamento\": 36,\n",
    "  \"deslizamentos\": 9,\n",
    "  \"deslizamento de terra\": 5\n",
    "}\n",
    "```\n",
    "\n",
    "Esses termos s√£o convertidos para o formato URL (espa√ßos viram hifens `-`).\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Ambiente Controlado\n",
    "\n",
    "A prepara√ß√£o autom√°tica realiza as seguintes a√ß√µes:\n",
    "\n",
    "* Instala e atualiza bibliotecas essenciais via `pip`\n",
    "* Verifica presen√ßa do Chrome e ChromeDriver e baixa automaticamente, se necess√°rio\n",
    "* Usa uma vers√£o espec√≠fica e port√°til do Chrome (modo headless ativado)\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Licen√ßa\n",
    "\n",
    "Este projeto √© de uso livre para fins educacionais, de pesquisa e jornalismo de dados.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5164bae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "    from selenium import webdriver\n",
    "\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.wait import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    if input(\"Deseja rodar prepara√ß√£o de ambiente (RECOMENDADO PARA PRIMEIRA VEZ): [sim/n√£o]\") in [\"sim\", \"Sim\", \"S\", \"s\"]:\n",
    "        raise Exception(\"Prepara√ß√£o de ambiente solicitada\")\n",
    "except:\n",
    "    print(\"Configurando ambiente\")\n",
    "    \n",
    "    import os\n",
    "    import subprocess\n",
    "    \n",
    "    print(\"Checking for not installed packages...\")\n",
    "    \n",
    "    result = subprocess.run([\"pip\", \"list\"], stdout=subprocess.PIPE, text=True)\n",
    "\n",
    "    if not all([lib in result.stdout for lib in [\"selenium\",\"wget\",\"pandas\",\"openpyxl\", \"beautifulsoup4\"]]):\n",
    "        print(\"Installing packages...\")\n",
    "        os.system(\"pip install --upgrade selenium wget pandas openpyxl beautifulsoup4\")\n",
    "    \n",
    "    print(\"All packages are installed!\")\n",
    "    \n",
    "    \n",
    "    print(\"Checking for outdated packages...\")\n",
    "    result = subprocess.run([\"pip\", \"list\", \"--outdated\"], stdout=subprocess.PIPE, text=True)\n",
    "    \n",
    "    if any([lib in result.stdout for lib in [\"selenium\",\"wget\",\"pandas\",\"openpyxl\", \"beautifulsoup4\"]]):\n",
    "        print(\"Updating packages...\")\n",
    "        os.system(\"pip install --upgrade selenium wget pandas openpyxl beautifulsoup4\")\n",
    "\n",
    "    print(\"All packages are updated!\")\n",
    "    \n",
    "    import wget\n",
    "    import zipfile\n",
    "    \n",
    "    if \"chromedriver\" not in os.listdir():\n",
    "        print(\"Downloading chromedriver\")\n",
    "        filename = wget.download(\"https://storage.googleapis.com/chrome-for-testing-public/134.0.6998.165/win64/chromedriver-win64.zip\")\n",
    "        with zipfile.ZipFile(f\"./{filename}\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"./chromedriver\")\n",
    "    else:\n",
    "        print(\"Chromedriver found!\")\n",
    "    \n",
    "    if \"chrome\" not in os.listdir():\n",
    "        print(\"Downloading chrome\")\n",
    "        filename = wget.download(\"https://storage.googleapis.com/chrome-for-testing-public/134.0.6998.165/win64/chrome-win64.zip\")\n",
    "        with zipfile.ZipFile(f\"./{filename}\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"./chrome\")\n",
    "    else:\n",
    "        print(\"Chrome found!\")\n",
    "\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.support.wait import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "options.binary_location = \"./chrome/chrome-win64/chrome.exe\"\n",
    "\n",
    "driverpath = Service(\"./chromedriver/chromedriver-win64/chromedriver.exe\")\n",
    "\n",
    "options.add_argument('--headless=new')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')  # Evita problemas de mem√≥ria compartilhada\n",
    "options.add_argument('--disable-web-security')\n",
    "options.add_argument('--disable-site-isolation-trials')\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--allow-running-insecure-content')\n",
    "options.add_argument('--disable-notifications')\n",
    "\n",
    "options.page_load_strategy = 'eager'\n",
    "\n",
    "driver = webdriver.Chrome(service=driverpath, options=options)\n",
    "\n",
    "def articleFormatter(article, tag): \n",
    "    return {\n",
    "        \"title\": article.find(\"h3\").text.strip(),\n",
    "        \"link\": article.find(\"a\").get_attribute_list(\"href\")[0],\n",
    "        \"data\": article.find(\"div\", class_=\"date\").text.strip(),\n",
    "        \"tag\": tag\n",
    "    }\n",
    "\n",
    "def SCfilter(article):\n",
    "    cidades_sc1 = pd.read_excel('./planilhas/cidade_sc1.xlsx')\n",
    "    \n",
    "    for key in [\" sc \", \"santa catarina\", \" sc\", \"sc \"]:\n",
    "        if key in article[\"title\"].lower():\n",
    "            return True\n",
    "        \n",
    "        \n",
    "    for key in [\"-sc-\", \"santa-catarina\", \"-sc\", \"sc-\"]:\n",
    "        if key in article[\"link\"].split(\"/\")[-1].lower():\n",
    "            return True\n",
    "\n",
    "    for cidade in cidades_sc1[\"MUNICIPIO\"]:\n",
    "        if cidade.lower() in article[\"title\"].lower() or cidade.lower() in article[\"link\"].split(\"/\")[-1].lower():\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def getNewsByTags(tags):\n",
    "    global driver\n",
    "    allNews = []\n",
    "\n",
    "    pageCounter = 0\n",
    "    for tag in tags.keys():\n",
    "        for page in range(tags[tag]):\n",
    "            if pageCounter % 10 == 0:\n",
    "                print(f\"\\nPlanilha salva com {len(allNews)} not√≠cias para backup...\")\n",
    "                storeAsExcel(allNews)\n",
    "                print(\"Salvo\\n\")\n",
    "            \n",
    "            acessed = False\n",
    "            \n",
    "            while not acessed:\n",
    "                try:\n",
    "                    driver.get(f\"https://www.nsctotal.com.br/tag/{tag}?page={page}\")\n",
    "                    acessed = True\n",
    "                except:\n",
    "                    print(\"Erro ao acessar a p√°gina, reiniciando navegador...\")\n",
    "                    driver.quit()\n",
    "                    driver = webdriver.Chrome(service=driverpath, options=options)\n",
    "    \n",
    "            try:\n",
    "                WebDriverWait(driver, 10).until( EC.presence_of_element_located( (By.CLASS_NAME, \"date\") ) )\n",
    "                driver.implicitly_wait(5)\n",
    "            except:\n",
    "                continue            \n",
    "            \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            news = soup.find_all('div', class_='featured-news-thumb')\n",
    "            \n",
    "            parsedNews = [articleFormatter(article, tag) for article in news]\n",
    "\n",
    "            parsedNews = list(filter(SCfilter, parsedNews))\n",
    "            \n",
    "            allNews += parsedNews\n",
    "            print(f\"\\nNoticias coletadas: {len(parsedNews)}\\nTag: {tag}\\nP√°gina:{page}\\nTotal: {len(allNews)}\\n\")\n",
    "\n",
    "            pageCounter += 1\n",
    "        \n",
    "        \n",
    "            \n",
    "    return allNews\n",
    "\n",
    "def storeAsExcel(data):\n",
    "    rows = list(map(lambda article: article.values(), data))\n",
    "    df = pd.DataFrame(rows, columns=[\"title\", \"link\", \"data\", \"tag\"])\n",
    "    \n",
    "    print(f\"N√∫mero de noticias com duplicados: {len(df)}\")\n",
    "    \n",
    "    df = df.drop(\"tag\", axis=1)\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    print(f\"N√∫mero de noticias sem duplicados: {len(df)}\")\n",
    "    \n",
    "    df.to_excel(\"./planilhas/noticias.xlsx\", index=False)\n",
    "    \n",
    "\n",
    "searchReference = {\n",
    "    \"chuvas\": 52,\n",
    "    \"chuva em sc\": 35,\n",
    "    \"chuvas em sc\": 60,\n",
    "    \"chuva\": 395,\n",
    "    \"chuva forte\": 5,\n",
    "    \"chuvarada\": 11,\n",
    "    \"temporal\": 109,\n",
    "    \"tempestade\": 21,\n",
    "    \"ciclone\": 32,\n",
    "    \"ciclone bomba\": 5,\n",
    "    \"ciclone extratropical\": 6,\n",
    "    \"previsao do tempo\": 708,\n",
    "    \"frente fria\": 9,\n",
    "    \"enchente\": 92,\n",
    "    \"enchentes\": 22,\n",
    "    \"alagamento\": 61,\n",
    "    \"alagamentos\": 20,\n",
    "    \"deslizamento\": 36,\n",
    "    \"deslizamentos\": 9,\n",
    "    \"deslizamento de terra\": 5\n",
    "}\n",
    "\n",
    "searchReference = {tag.replace(\" \", \"-\"): searchReference[tag] for tag in searchReference}\n",
    "\n",
    "data = getNewsByTags(searchReference)\n",
    "\n",
    "print(data)\n",
    "\n",
    "storeAsExcel(data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
